{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec # make use of pretrained embeddings\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils # for converting labels vectors to matrices in multi-class cases\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"../annotations.json\")\n",
    "\n",
    "# filter out bugs and empty relations\n",
    "data = data[(data.relation != \"Bug\") & (data.relation != \"\")]\n",
    "\n",
    "# set labels other than precedence to \"None\"\n",
    "label_to_value = {\n",
    "    'E1 precedes E2': 1,\n",
    "    'E2 precedes E1': 2,\n",
    "    'None': 0,\n",
    "    'E1 subsumes E2': 0, \n",
    "    'E2 subsumes E1': 0, \n",
    "    'Equivalent': 0, \n",
    "    'Other': 0\n",
    "}\n",
    "\n",
    "data.relation = data.relation.replace(label_to_value)\n",
    "\n",
    "print(set(data.relation.values))\n",
    "\n",
    "# TODO: split the data evenly among classes (training, dev, and test)\n",
    "# TODO: perform 5-fold cross validation with where each fold has the the three classes represented\n",
    "# text as input\n",
    "x = data.text.values\n",
    "# relations as labels\n",
    "y = data.relation.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789'\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot, base_filter, Tokenizer\n",
    "\n",
    "# remove numbers, punct (except for _), etc.\n",
    "custom_filter = \"!#$%&()*+,-./:;<=>?@[\\]^_`{|}~0123456789'\" \n",
    "print(custom_filter)\n",
    "tk = Tokenizer(\n",
    "    # the maximum num. of words to retain\n",
    "    nb_words=None,\n",
    "    # the characters to filter out from the text\n",
    "    filters=custom_filter,\n",
    "    # whether or not to convert the text to lowercase\n",
    "    lower=True,\n",
    "    # the character to split on\n",
    "    split=\" \",\n",
    "    # whether or not to treat each character as a word\n",
    "    char_level=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tk.fit_on_texts(x)\n",
    "#one_hot(text, n, filters=base_filter(), lower=True, split=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text into sequences of term indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tk.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# the maximum size of a sequence\n",
    "max_len = 200\n",
    "\n",
    "x = sequence.pad_sequences(x, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "w2v_data = \"../pmc-openaccess-w2v.bin\"\n",
    "def create_embeddings_weights(w2v_vectors_file, tokenizer):\n",
    "    word2index = tk.word_index\n",
    "    # reverse index\n",
    "    index2word = {i:w for (w,i) in tk.word_index.items()}\n",
    "    max_size = len(index2word) + 1\n",
    "    # load w2v model\n",
    "    w2v = Word2Vec.load_word2vec_format(w2v_vectors_file, binary=True)\n",
    "    word_vector_dims = w2v.vector_size\n",
    "    embedding_weights = np.zeros((max_size, word_vector_dims))\n",
    "    \n",
    "    for i,w in index2word.items():\n",
    "        try:\n",
    "            embedding_weights[i,:] = w2v[w]\n",
    "        except:\n",
    "            print(\"{} not found\".format(w))\n",
    "    return (w2v, embedding_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained word embeddings\n",
    "\n",
    "We can initialize our network with pretrained word embeddings.  Here, we use embeddings generated using a `word2vec` model that was trained on the open access subset of PubMed retrieved in **????**, which contains over a million (**TODO: get exact number**) papers.  \n",
    "\n",
    "In preparing the text for `word2vec`, we chose to ignore the following sections in the `nxml` files: \"references\", \"materials\", \"methods\", and \"supplementary-material\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" not found\n",
      "atmmutation not found\n",
      "ralas not found\n",
      "piasgamma not found\n",
      "kgamma not found\n",
      "gammah not found\n",
      "pkczetaii not found\n",
      "ikappak not found\n",
      "hamamori not found\n",
      "gigoux not found\n",
      "gresko not found\n",
      "deltarbd not found\n",
      "tovok not found\n",
      "raswt not found\n"
     ]
    }
   ],
   "source": [
    "# get pretrained embeddings from w2v model\n",
    "(w2v, pretrained_weights) = create_embeddings_weights(w2v_data, tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set layer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max features: 2990\n"
     ]
    }
   ],
   "source": [
    "# the maximum number of features to retain\n",
    "max_features = len(tk.word_index) + 1 # for mask\n",
    "print(\"Max features: {}\".format(max_features))\n",
    "\n",
    "# the number of samples to use for one update\n",
    "batch_size = 32\n",
    "# \n",
    "hidden_size = w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare classes\n",
    "\n",
    "We have 3 possible labels (\"E1 precedes E2\", \"E2 precedes E1\", or \"None\"), so we need to convert our length-normalized class label vectors to matrices of size $n$ classes.\n",
    "\n",
    "**Important**: _if you don't do this, the model will not learn!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y = np_utils.to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 780 samples, validate on 156 samples\n",
      "Epoch 1/8\n",
      "780/780 [==============================] - 20s - loss: 0.6662 - acc: 0.7769 - val_loss: 0.3151 - val_acc: 0.8846\n",
      "Epoch 2/8\n",
      "780/780 [==============================] - 21s - loss: 0.5016 - acc: 0.8256 - val_loss: 0.2712 - val_acc: 0.8846\n",
      "Epoch 3/8\n",
      "780/780 [==============================] - 21s - loss: 0.4664 - acc: 0.8256 - val_loss: 0.2325 - val_acc: 0.8974\n",
      "Epoch 4/8\n",
      "780/780 [==============================] - 20s - loss: 0.4174 - acc: 0.8295 - val_loss: 0.2212 - val_acc: 0.9295\n",
      "Epoch 5/8\n",
      "780/780 [==============================] - 20s - loss: 0.3893 - acc: 0.8462 - val_loss: 0.1961 - val_acc: 0.9103\n",
      "Epoch 6/8\n",
      "780/780 [==============================] - 20s - loss: 0.3567 - acc: 0.8474 - val_loss: 0.1841 - val_acc: 0.9103\n",
      "Epoch 7/8\n",
      "780/780 [==============================] - 19s - loss: 0.3182 - acc: 0.8769 - val_loss: 0.1442 - val_acc: 0.9359\n",
      "Epoch 8/8\n",
      "780/780 [==============================] - 19s - loss: 0.3258 - acc: 0.8718 - val_loss: 0.1344 - val_acc: 0.9359\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# build the embeddings layer\n",
    "embeddings = Embedding(\n",
    "    input_dim=max_features, \n",
    "    output_dim=hidden_size, \n",
    "    input_length=max_len, \n",
    "    W_regularizer=None,\n",
    "    #weights=None,\n",
    "    # use pretrained vectors\n",
    "    weights=[pretrained_weights],\n",
    "    dropout=0.2\n",
    ")\n",
    "model.add(embeddings)\n",
    "# build the lstm layer\n",
    "lstm = LSTM(\n",
    "    #input_dim=max_features,\n",
    "    output_dim=hidden_size, \n",
    "    dropout_W=0.2, \n",
    "    dropout_U=0.2, \n",
    "    return_sequences=False\n",
    ")\n",
    "model.add(lstm)\n",
    "model.add(Dropout(0.5))\n",
    "# size should be equal to the number of classes\n",
    "model.add(Dense(num_classes))\n",
    "# at the end of the day, we only want one label per input (hence softmax)\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# add early stopping to help avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='rmsprop', \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    # input\n",
    "    x, \n",
    "    # target labels\n",
    "    y=y, \n",
    "    # how many examples to consider at once\n",
    "    batch_size=batch_size, \n",
    "    # the number of epochs to train\n",
    "    nb_epoch=8,\n",
    "    # 0 for no logging, 1 for progress bar logging, 2 for one log line per epoch\n",
    "    verbose=1,\n",
    "    # the validation data to use,\n",
    "    #validation_data=(x_dev, y_dev),\n",
    "    # how much data to reserve for validation (takes n% starting at the end of the dataset)\n",
    "    validation_split=0.2,\n",
    "    # should the training data be shuffled?\n",
    "    shuffle=True,\n",
    "    # dict mapping classes to weight for scaling in loss function\n",
    "    class_weight=None,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.216140824633\n",
      "Test accuracy: 0.894871795177\n"
     ]
    }
   ],
   "source": [
    "performance = model.evaluate(x, y, batch_size=batch_size, verbose=0)\n",
    "loss, accuracy = performance[0], performance[-1]\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "We can display a simple graph of the network's architecture using `dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 211.00 410.00\" width=\"211pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 207.004,-406 207.004,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 6545415640 -->\n",
       "<g class=\"node\" id=\"node1\"><title>6545415640</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 203.004,-401.5 203.004,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-379.3\">embedding_input_1 (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 6545415920 -->\n",
       "<g class=\"node\" id=\"node2\"><title>6545415920</title>\n",
       "<polygon fill=\"none\" points=\"16.7139,-292.5 16.7139,-328.5 186.29,-328.5 186.29,-292.5 16.7139,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-306.3\">embedding_1 (Embedding)</text>\n",
       "</g>\n",
       "<!-- 6545415640&#45;&gt;6545415920 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>6545415640-&gt;6545415920</title>\n",
       "<path d=\"M101.502,-365.313C101.502,-357.289 101.502,-347.547 101.502,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-338.529 101.502,-328.529 98.0021,-338.529 105.002,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6545416032 -->\n",
       "<g class=\"node\" id=\"node3\"><title>6545416032</title>\n",
       "<polygon fill=\"none\" points=\"49.3623,-219.5 49.3623,-255.5 153.642,-255.5 153.642,-219.5 49.3623,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-233.3\">lstm_1 (LSTM)</text>\n",
       "</g>\n",
       "<!-- 6545415920&#45;&gt;6545416032 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>6545415920-&gt;6545416032</title>\n",
       "<path d=\"M101.502,-292.313C101.502,-284.289 101.502,-274.547 101.502,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-265.529 101.502,-255.529 98.0021,-265.529 105.002,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6568442792 -->\n",
       "<g class=\"node\" id=\"node4\"><title>6568442792</title>\n",
       "<polygon fill=\"none\" points=\"34.9829,-146.5 34.9829,-182.5 168.021,-182.5 168.021,-146.5 34.9829,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-160.3\">dropout_1 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 6545416032&#45;&gt;6568442792 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>6545416032-&gt;6568442792</title>\n",
       "<path d=\"M101.502,-219.313C101.502,-211.289 101.502,-201.547 101.502,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-192.529 101.502,-182.529 98.0021,-192.529 105.002,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6579252248 -->\n",
       "<g class=\"node\" id=\"node5\"><title>6579252248</title>\n",
       "<polygon fill=\"none\" points=\"46.6587,-73.5 46.6587,-109.5 156.345,-109.5 156.345,-73.5 46.6587,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-87.3\">dense_1 (Dense)</text>\n",
       "</g>\n",
       "<!-- 6568442792&#45;&gt;6579252248 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>6568442792-&gt;6579252248</title>\n",
       "<path d=\"M101.502,-146.313C101.502,-138.289 101.502,-128.547 101.502,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-119.529 101.502,-109.529 98.0021,-119.529 105.002,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 6583443408 -->\n",
       "<g class=\"node\" id=\"node6\"><title>6583443408</title>\n",
       "<polygon fill=\"none\" points=\"22.9414,-0.5 22.9414,-36.5 180.062,-36.5 180.062,-0.5 22.9414,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101.502\" y=\"-14.3\">activation_1 (Activation)</text>\n",
       "</g>\n",
       "<!-- 6579252248&#45;&gt;6583443408 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>6579252248-&gt;6583443408</title>\n",
       "<path d=\"M101.502,-73.3129C101.502,-65.2895 101.502,-55.5475 101.502,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"105.002,-46.5288 101.502,-36.5288 98.0021,-46.5289 105.002,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.visualize_util import model_to_dot\n",
    "from IPython.display import SVG\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# from keras.utils.visualize_util import plot\n",
    "# plot(model, to_file='model.png')\n",
    "# plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `LSTM` to classifier's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E1 precedes E2</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2 precedes E1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.63</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOTAL (macro)</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Class  Precision  Recall    F1  Support\n",
       "0  E1 precedes E2       0.13    0.37  0.19      122\n",
       "1  E2 precedes E1       0.00    0.00  0.00       16\n",
       "2            None       0.79    0.53  0.63      641\n",
       "3   TOTAL (macro)       0.31    0.30  0.27      779"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import compute_performance\n",
    "\n",
    "performance = compute_performance(\"results.tsv\")\n",
    "\n",
    "performance.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# def load_data():\n",
    "#     # load your data using this function\n",
    "\n",
    "# def create model():\n",
    "#     # create your model using this function\n",
    "\n",
    "# def train_and_evaluate__model(model, data[train], labels[train], data[test], labels[test)):\n",
    "#     model.fit...\n",
    "#     # fit and evaluate here.\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     n_folds = 10\n",
    "#     data, labels, header_info = load_data()\n",
    "#     skf = StratifiedKFold(labels, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "#     for i, (train, test) in enumerate(skf):\n",
    "#             print \"Running Fold\", i+1, \"/\", n_folds\n",
    "#             model = None # Clearing the NN.\n",
    "#             model = create_model()\n",
    "#             train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
